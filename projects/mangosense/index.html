<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!--   <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MangoSense: A time-series vision sensing dataset for mango tree segmentation and detection toward yield prediction</h1>
            <h2 style="color:red;" class="title is-3">Computers and Electronics in Agriculture (Impact Factor: 8.9) 2025</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://janaksinhven.github.io/">Janaksinh Ven</a><sup>1</sup>,</span>
              <span class="author-block">
                <a>Charu Sharma</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Azeemuddin Syed</a><sup>2</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Machine Learning Lab, IIIT Hyderabad,</span>
              <span class="author-block"><sup>2</sup>Electrical and Computer Engineering, Penn State Erie, The Behrend College, Pennsylvania, USA,</span>
            </div>

            <div class="column has-text-centered">
              <div class="">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href= "https://doi.org/10.1016/j.compag.2025.110524"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://janaksinhven.github.io/projects/mangosense/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/JanaksinhVen/mangosense-yolo-sam-annotator.git/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3">Architecture</h2> -->
          <div class="publication-video">
            <img src="./static/images/method_diagram.jpg" class="interpolation-image" alt="architecture." />
          </div>
        </div>
      </div>
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Mangoes hold significant economic and cultural importance in India and globally,
               making accurate yield prediction crucial for optimizing domestic consumption,
                enhancing international trade, and supporting farmers in decision-making.
                 Traditional yield estimation methods, such as manual counting, are labor-intensive,
                  error-prone, and impractical for large-scale orchards, necessitating automated solutions.
                   This study presents a novel time-series vision dataset for mango yield prediction,
                    capturing images from 12 trees across eight spatial orientations (SW, S, SE, E, NE, N, NW, W)
                     over 2.5 months to analyze fruit growth and flowering patterns. A high-resolution image
                      dataset was created and annotated using a semi-automatic pipeline integrating YOLO for object
                       detection and SAM for precise segmentation, significantly reducing manual annotation efforts.
                        Benchmarking was conducted using state-of-the-art deep learning models for segmentation
                         (DeepLabV3+, PSPNet, SegFormer, Swin-S, YOLO+SAM) and detection (Mask R-CNN, Faster R-CNN, DETR, YOLO).
                          For flower segmentation, Swin-S achieved the best results with 67.35 IoU, followed closely by SegFormer
                           with 66.44 IoU, while for fruitlet segmentation, YOLO+SAM obtained the highest IoU of 78.97. In detection
                            tasks, YOLO achieved the best performance for both flowers and fruitlets, with 63.8 mAP50 and 80.5 mAP50,
                             respectively. Additionally, segmentation is identified as a suitable approach for flower feature extraction
                              in yield prediction models, with SegFormer emerging as a strong choice due to its lower computational cost.
                               Furthermore, this study discusses the relationship between wind direction patterns and the flower-to-fruit
                                conversion ratio, challenging previous research that attributed yield variations solely to canopy structure
                                 differences with respect to orientation.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Architecture</h2>
          <div class="publication-video">
            <img src="./static/images/architecture.png" class="interpolation-image" alt="architecture." />
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>


  <!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>